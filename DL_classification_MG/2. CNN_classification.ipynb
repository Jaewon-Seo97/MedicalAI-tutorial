{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install silence_tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import silence_tensorflow.auto\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting numpy data\n",
    "\n",
    "path_save = os.path.join(os.path.abspath('.'), 'npy')\n",
    "train_x = np.load(os.path.join(path_save, 'train_img.npy'))\n",
    "test_x = np.load(os.path.join(path_save, 'test_img.npy'))\n",
    "train_y = np.load(os.path.join(path_save, 'train_lab.npy'))\n",
    "test_y = np.load(os.path.join(path_save, 'test_lab.npy'))\n",
    "\n",
    "print('Data shape')\n",
    "print(f'Train img: {train_x.shape} | Train lab: {train_y.shape}')\n",
    "print(f'Test img: {test_x.shape} | Test lab: {test_y.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sample\n",
    "n_sample = 5\n",
    "class_dict = {'normal': 0,\n",
    "                'abnormal': 1}\n",
    "\n",
    "for n in range(1, len(train_x), len(train_x)//n_sample):\n",
    "\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(f'{list(class_dict.keys())[train_y[n].argmax()]}')\n",
    "    plt.imshow(train_x[n],cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simple_CNNmodel(input_img, base=32, scale=2, n_layers=6):\n",
    "    # 입력 이미지 텐서를 x에 할당함\n",
    "    x = input_img\n",
    "\n",
    "    # n_layers 수 만큼 CNN 층을 추가함\n",
    "    for n in range(n_layers):\n",
    "        # 1번째 Convolutional Layer: 필터 수는 base에 scale을 n번 지수로 적용한 값임\n",
    "        x = Conv2D(((scale)**n) * base, 3, activation=None, padding='same', kernel_initializer='he_normal')(x)\n",
    "        # Batch Normalization 적용\n",
    "        x = BatchNormalization()(x)\n",
    "        # ReLU 활성화 함수 적용\n",
    "        x = Activation(activation='relu')(x)\n",
    "\n",
    "        # 2번째 Convolutional Layer: 필터 수 동일하게 적용함\n",
    "        x = Conv2D(((scale)**n) * base, 3, activation=None, padding='same', kernel_initializer='he_normal')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation(activation='relu')(x)\n",
    "\n",
    "        # 마지막 층 제외, 각 층의 출력을 절반 크기로 다운샘플링함\n",
    "        if n != n_layers:\n",
    "            x = MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        else:\n",
    "            pass  # 마지막 층에서 MaxPooling2D 미적용\n",
    "\n",
    "    \n",
    "    out = GlobalAveragePooling2D(name=\"avg_pool\")(x)    # Global Average Pooling으로 최종 특징 맵을 하나의 벡터로 변환함\n",
    "\n",
    "    \n",
    "    out = Dense(128, activation=\"relu\")(out)    # 완전 연결(Dense) 층을 통해 출력 크기를 128로 줄임\n",
    "    out = BatchNormalization()(out)\n",
    "\n",
    "    \n",
    "    out = Dense(64, activation=\"relu\")(out) # 두 번째 완전 연결(Dense) 층 추가해 출력 크기를 64로 줄임\n",
    "    out = BatchNormalization()(out)\n",
    "    \n",
    "    out = Dropout(0.3)(out)   # Dropout을 적용해 과적합 방지\n",
    "\n",
    "    # 최종 출력 층: n_class 개수 클래스에 대해 softmax 활성화 함수 적용하여 확률 계산\n",
    "    out = Dense(n_class, activation=\"softmax\")(out)\n",
    "    \n",
    "    # 모델 정의 및 입력과 출력 지정함\n",
    "    model = Model(inputs=input_img, outputs=out)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classification model using Simple CNN model\n",
    "\n",
    "path_save = os.path.join(os.path.abspath('.'), 'Result', 'model_simple')\n",
    "os.makedirs(path_save, exist_ok=True)\n",
    "\n",
    "## parameter setting\n",
    "n_class = 2         \n",
    "imageSize = 512     \n",
    "lr = 0.0001\n",
    "epochs = 50\n",
    "batch = 10\n",
    "loss_function = \"categorical_crossentropy\"\n",
    "\n",
    "## model build\n",
    "input_img = Input(shape=(imageSize,imageSize,1))\n",
    "\n",
    "model = Simple_CNNmodel(input_img, n_class)\n",
    "model.summary()         # Check model structure\n",
    "\n",
    "## model compile\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "model.compile(\n",
    "        optimizer=optimizer, loss=loss_function, metrics=[\"accuracy\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Callback\n",
    "checkpointer = ModelCheckpoint(filepath=os.path.join(path_save,f'best_model.h5'), \n",
    "                                verbose=1, \n",
    "                                save_weights_only=True, \n",
    "                                save_best_only=True, \n",
    "                                monitor='val_loss', \n",
    "                                save_freq='epoch')\n",
    "\n",
    "# ReduceLROnPlateau: Keras에서 제공하는 콜백(callback) 중 하나, \n",
    "# 학습률(learning rate)을 동적으로 조정하는 기능을 제공. \n",
    "# 주로 학습 중에 검증 손실(validation loss)이 더 이상 개선되지 않을 때 학습률을 감소시키는데 사용\n",
    "\"\"\"\n",
    "monitor: 모니터링할 지표\n",
    "\n",
    "factor: 학습률을 줄일 비율 => factor=0.1일 경우 현재 학습률의 10%로 줄입니다.\n",
    "\n",
    "patience: 학습률 감소 판단 전 지켜볼 에포크(epoch) 수\n",
    "\n",
    "min_lr: 학습률이 줄어들 수 있는 최소값\n",
    "\n",
    "min_delta: 감소 판단 기준의 최소 변화량 => 이 값보다 작은 손실 개선은 개선 없음으로 간주\n",
    "\n",
    "verbose: 출력 여부\n",
    "\"\"\"\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10, min_lr=0, min_delta=0.001, verbose=1)\n",
    "callbacks_list = [reduce_lr, checkpointer]\n",
    "\n",
    "\n",
    "## training model\n",
    "model.fit(train_x, train_y,\n",
    "            steps_per_epoch=len(train_x) // batch, epochs=epochs,\n",
    "            validation_split=0.2,\n",
    "            callbacks=callbacks_list,\n",
    "            shuffle=True\n",
    "                        )\n",
    "\n",
    "## saving last model weight\n",
    "model.save_weights(os.path.join(path_save,f'last_model.h5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation using test data\n",
    "## Test data => 검증을 위하여 학습에 활용하지 않은 별도의 데이터\n",
    "_loss, _acc = model.evaluate(train_x, train_y, batch_size=10, verbose=1)\n",
    "print('Last model accuracy')\n",
    "print(f'loss: {_loss:0.3f} | accuracy: {_acc:0.3f}')\n",
    "_loss, _acc = model.evaluate(test_x, test_y, batch_size=10, verbose=1)\n",
    "print('Last model accuracy')\n",
    "print(f'loss: {_loss:0.3f} | accuracy: {_acc:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using best model\n",
    "### 모델 구조에서 학습을 통해 생성된 모델의 가중치 적용\n",
    "model.load_weights(os.path.join(path_save,f'best_model.h5'))\n",
    "_loss, _acc = model.evaluate(test_x, test_y, batch_size=10, verbose=1)\n",
    "print('Best model accuracy')\n",
    "print(f'loss: {_loss:0.3f} | accuracy: {_acc:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking fail data\n",
    "test_result = model.predict(test_x, batch_size=1)\n",
    "\n",
    "for a in range(len(test_result)):\n",
    "    if test_result[a].argmax()!=test_y[a].argmax():\n",
    "        plt.title(f'GT: {list(class_dict.keys())[test_y[a].argmax()]} | Result: {list(class_dict.keys())[test_result[a].argmax()]}')\n",
    "        plt.imshow(test_x[a], cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"fig_source/confusion_matrix.jpg\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix 확인\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "test_th = np.where(test_result > 0.5, 1, 0)\n",
    "\n",
    "cm = confusion_matrix(test_y.argmax(axis=1), test_result.argmax(axis=1))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['normal', 'abnormal'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(test_y.argmax(axis=1), test_result.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawing_ROCcurve(test_y, test_result, path_save):\n",
    "\n",
    "    def compute_roc_auc(y_true, y_score):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        return fpr, tpr, auc(fpr, tpr)\n",
    "\n",
    "    n_classes = test_y.shape[-1]\n",
    "    fpr, tpr, roc_auc = {}, {}, {}\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], roc_auc[i] = compute_roc_auc(test_y[:, i], test_result[:, i])\n",
    "\n",
    "    fpr[\"micro\"], tpr[\"micro\"], roc_auc[\"micro\"] = compute_roc_auc(test_y.ravel(), test_result.ravel())\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "    mean_tpr = np.mean([np.interp(all_fpr, fpr[i], tpr[i]) for i in range(n_classes)], axis=0)\n",
    "\n",
    "    fpr[\"macro\"], tpr[\"macro\"], roc_auc[\"macro\"] = all_fpr, mean_tpr, auc(all_fpr, mean_tpr)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.plot(fpr[\"micro\"], tpr[\"micro\"], label=f\"micro-average (auc = {roc_auc['micro']:.2f})\", color=\"maroon\", linestyle=\":\", linewidth=4)\n",
    "    plt.plot(fpr[\"macro\"], tpr[\"macro\"], label=f\"macro-average (auc = {roc_auc['macro']:.2f})\", color=\"navy\", linestyle=\":\", linewidth=4)\n",
    "\n",
    "    colors = ['blue', 'yellow', 'green', 'red', 'purple', 'orange', 'pink', 'brown', 'gray', 'cyan']\n",
    "    for i, color in zip(range(n_classes), colors[:n_classes]):\n",
    "        plt.plot(fpr[i], tpr[i], lw=2, color=color, label=f\"ROC curve (auc = {roc_auc[i]:.2f})\")\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.xlabel(\"False Positive Rate\", fontsize=20)\n",
    "    plt.ylabel(\"True Positive Rate\", fontsize=20)\n",
    "    plt.title(\"Receiver operating characteristic curves\", fontsize=20)\n",
    "    plt.legend(loc=\"lower right\", fontsize=14)\n",
    "    plt.savefig(f'{path_save}/ROC_curve.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drawing_ROCcurve(test_y, test_result, path_save)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f0642c3d6a6210ac9dcf3acf256e659a1278243731471d18f7b51f84222164b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
