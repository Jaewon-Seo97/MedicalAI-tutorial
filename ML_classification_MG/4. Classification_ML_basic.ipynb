{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "from sklearn.feature_selection import VarianceThreshold, RFE\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "df = pd.read_csv(\"data/Radiomic_features_all.csv\" , sep = \",\")\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting parameter\n",
    "rand_seed = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data split\n",
    "X = df.drop('label', axis=1)  # Features\n",
    "Y = df['label']  # Target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=rand_seed, stratify=Y)\n",
    "\n",
    "\n",
    "print(f'Train: {X_train.shape} | Test: {X_test.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying data using RandomForest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "base_model = RandomForestClassifier(random_state=rand_seed)\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 1. base model\n",
    "print(\"Baseline Accuray without Feature Selection\")\n",
    "print(f\"Train: {base_model.score(X_train, y_train):.4f}\")\n",
    "print(f\"Test : {base_model.score(X_test, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Variance Thresholding\n",
    "variance_thresh = VarianceThreshold(threshold=0.1)  # 분산이 0.1 이하인 특징 제거\n",
    "X_train_var = variance_thresh.fit_transform(X_train)\n",
    "X_test_var = variance_thresh.transform(X_test)\n",
    "\n",
    "# 선택된 특징 확인\n",
    "var_selected_features = X_train.columns[variance_thresh.get_support()]\n",
    "print(f\"Selected Features after Variance Thresholding: {list(var_selected_features)}\")\n",
    "\n",
    "# 선택된 특징 적용\n",
    "var_model = RandomForestClassifier(random_state=rand_seed)\n",
    "var_model.fit(X_train_var, y_train)\n",
    "print(\"Accuray with Feature Selection: Varience thresholding\")\n",
    "print(f\"Train: {var_model.score(X_train_var, y_train):.4f}\")\n",
    "print(f\"Test : {var_model.score(X_test_var, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Recursive Feature Elimination (RFE)\n",
    "rfe = RFE(estimator=RandomForestClassifier(random_state=rand_seed), n_features_to_select=10)  # 상위 10개 특징 선택\n",
    "X_train_rfe = rfe.fit_transform(X_train, y_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "# 선택된 특징 확인\n",
    "rfe_selected_features = X_train.columns[rfe.get_support()]\n",
    "print(f\"Selected Features after RFE: {list(rfe_selected_features)}\")\n",
    "\n",
    "# 선택된 특징 적용\n",
    "rfe_model = RandomForestClassifier(random_state=rand_seed)\n",
    "rfe_model.fit(X_train_rfe, y_train)\n",
    "print(\"Accuray with Feature Selection: Recursive Feature Elimination\")\n",
    "print(f\"Train: {rfe_model.score(X_train_rfe, y_train):.4f}\")\n",
    "print(f\"Test : {rfe_model.score(X_test_rfe, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Tree-based Feature Importance\n",
    "base_model.fit(X_train, y_train)\n",
    "importances = base_model.feature_importances_\n",
    "threshold = np.mean(importances)  # 평균 이상의 중요도를 가진 특징 선택\n",
    "tree_selected_features = X_train.columns[importances > threshold]\n",
    "X_train_tree = X_train[tree_selected_features]\n",
    "X_test_tree = X_test[tree_selected_features]\n",
    "\n",
    "# 선택된 특징 확인\n",
    "print(f\"Selected Features after Tree-based Feature Importance: {list(tree_selected_features)}\")\n",
    "\n",
    "# 선택된 특징 적용\n",
    "base_model.fit(X_train_tree, y_train)\n",
    "print(\"Accuray with Feature Selection: Tree-based Feature Importance\")\n",
    "print(f\"Train: {base_model.score(X_train_tree, y_train):.4f}\")\n",
    "print(f\"Test : {base_model.score(X_test_tree, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "with open('model_rf.pkl', 'wb') as f:\n",
    "    pickle.dump(base_model, f)\n",
    "    \n",
    "# 선별된 특징들로 구성된 데이터셋 저장\n",
    "selected_df = df[tree_selected_features]\n",
    "selected_df['label'] = df['label']  # label 열 추가\n",
    "selected_df.to_csv(f'./data/selected_tree.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 성능 결과 확인\n",
    "from sklearn.metrics import classification_report, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "print(classification_report(y_test,base_model.predict(X_test_tree)))\n",
    "\n",
    "# Comufsion matrix 확인\n",
    "\n",
    "cm = confusion_matrix(y_test,base_model.predict(X_test_tree))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                              display_labels=['benign', 'malignant'])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import roc_auc_score, auc, roc_curve, RocCurveDisplay\n",
    "\n",
    "y_pred = base_model.predict(X_test_tree)\n",
    "print(classification_report(y_test,y_pred))\n",
    "\n",
    "label_binarizer = LabelBinarizer().fit(y_train)\n",
    "y_onehot_test = label_binarizer.transform(y_test)\n",
    "y_onehot_test.shape  # (n_samples, n_classes)\n",
    "y_pred = label_binarizer.transform(y_pred)\n",
    "\n",
    "n_classes = y_onehot_test.shape[-1]\n",
    "\n",
    "# store the fpr, tpr, and roc_auc for all averaging strategies\n",
    "fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_pred.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")\n",
    "\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "# Interpolate all ROC curves at these points\n",
    "mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "for i in range(n_classes):\n",
    "    mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "# Average it and compute AUC\n",
    "mean_tpr /= n_classes\n",
    "\n",
    "fpr[\"macro\"] = fpr_grid\n",
    "tpr[\"macro\"] = mean_tpr\n",
    "roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"micro\"],\n",
    "    tpr[\"micro\"],\n",
    "    label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "    color=\"deeppink\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    fpr[\"macro\"],\n",
    "    tpr[\"macro\"],\n",
    "    label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "    color=\"navy\",\n",
    "    linestyle=\":\",\n",
    "    linewidth=4,\n",
    ")\n",
    "\n",
    "colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "for class_id, color in zip(range(n_classes), colors):\n",
    "    RocCurveDisplay.from_predictions(\n",
    "        y_onehot_test[:, class_id],\n",
    "        y_pred[:, class_id],\n",
    "        name=f\"ROC curve for {class_id+1}\",\n",
    "        color=color,\n",
    "        ax=ax\n",
    "    )\n",
    "\n",
    "_ = ax.set(\n",
    "    xlabel=\"False Positive Rate\",\n",
    "    ylabel=\"True Positive Rate\",\n",
    "    title=\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sha_tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
